{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "이전 강의에서 모델 학습했던 코드를 다시 실행해서 학습된 모델을 만들어보겠습니다."
      ],
      "metadata": {
        "id": "ufHJYFhVTxPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# 2. 데이터셋 준비 (3개의 Feature를 가진 데이터)\n",
        "x = np.random.rand(100, 3)\n",
        "# 간단한 목표 변수 생성: y = 2*x1 - 3*x2 + 5*x3 + 1 + noise\n",
        "y = 2*x[:, 0] - 3*x[:, 1] + 5*x[:, 2] + 1 + np.random.randn(100)\n",
        "\n",
        "# PyTorch 텐서로 변환\n",
        "x_tensor = torch.from_numpy(x).float()\n",
        "y_tensor = torch.from_numpy(y).float()\n",
        "\n",
        "# Dataset과 DataLoader 준비\n",
        "dataset = TensorDataset(x_tensor, y_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)  # 데이터를 섞는 옵션을 지정\n",
        "\n",
        "# 3. 모델 정의 (Sequential 사용)\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(3, 5),  # 입력 차원 3, 중간 층 차원 5\n",
        "    nn.ReLU(),   # 비선형 특성 부여\n",
        "    nn.Linear(5, 4),  # 중간 층 차원 5, 또 다른 중간 층 차원 4\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(4, 1)   # 마지막 층 차원 4, 출력 차원 1\n",
        ")\n",
        "\n",
        "# 4. 손실 함수와 옵티마이저 설정\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# 5. 학습\n",
        "epochs = 200  # 학습을 위해 에포크 수를 늘림\n",
        "for epoch in range(epochs):\n",
        "    for inputs, targets in dataloader:\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        optimizer.zero_grad()  # gradient 0으로 초기화\n",
        "        loss.backward()   # gradient 계산\n",
        "        optimizer.step()   # weight 수정\n",
        "\n",
        "# 학습된 모델의 가중치 중 일부 확인\n",
        "list(model.parameters())[:2]  # 첫 번째 선형 층의 가중치와 편향만 출력\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Et83F_FhTl4u",
        "outputId": "184e3471-dd99-47b3-b8a5-6d68edb8c7d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[-0.4567, -0.5604, -0.4118],\n",
              "         [ 0.2682, -0.4997,  0.0448],\n",
              "         [ 0.3193, -0.3259, -0.2429],\n",
              "         [-0.5441, -0.3319, -0.4362],\n",
              "         [ 0.1056,  0.3903,  0.0975]], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([ 0.3382,  0.3751, -0.0424, -0.5101,  0.0540], requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 학습된 모델에 새로운 데이터를 넣고 모델의 예측값을 구해보겠습니다.  "
      ],
      "metadata": {
        "id": "P5SZSwwmT5Aq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0SZ8-0A3_nA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c8768a3-587c-4a10-fa72-a838f20d209d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.09780399 0.9475629  0.30694857]\n",
            " [0.91004443 0.34280536 0.18660139]\n",
            " [0.3295056  0.93059796 0.24628526]\n",
            " [0.9804526  0.39894387 0.29355928]\n",
            " [0.6420869  0.66092396 0.94132334]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[3.2750],\n",
              "        [3.3213],\n",
              "        [3.2761],\n",
              "        [3.3213],\n",
              "        [3.3000]])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# 새로운 데이터 생성 (3개의 특성을 가진 데이터)\n",
        "new_x = np.random.rand(5, 3).astype(np.float32)  # 예제로 5개의 새로운 데이터 포인트 생성\n",
        "new_x_tensor = torch.from_numpy(new_x)\n",
        "\n",
        "print(new_x)\n",
        "\n",
        "# 모델을 평가 모드로 설정 (필요한 경우, 예를 들어 Dropout이나 BatchNorm과 같은 특정 층의 동작을 평가 모드로 설정)\n",
        "model.eval()\n",
        "\n",
        "# 새로운 데이터에 대한 예측 수행\n",
        "with torch.no_grad():  # 기울기 계산을 비활성화하여 메모리 사용량을 줄이고 계산 속도를 높임\n",
        "    predictions = model(new_x_tensor)\n",
        "\n",
        "# 예측 결과 출력\n",
        "predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train과 eval을 반복하는 경우에는 model.eval() <--> model.train() 모드로 학습과 평가 모드 변환을 해줍니다"
      ],
      "metadata": {
        "id": "phuxyHjj4AeV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* torch.no_grad() : gradient 계산을 안 함으로 리소스 효율 향상, Inference의 경우 gradient 계산 필요 없으므로 주로 사용. with 문법과 함께 사용\n",
        "\n",
        "* torch.eval(), torch.train(): dropout, barchnorm등 train과 evaluation 단계에서 다른 연산을 하는 경우 전환\n",
        "\n",
        "* optimizer.zero_grad(): gradient 초기화"
      ],
      "metadata": {
        "id": "ib_c9i7ldmnc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sBGyxJt14EAK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}